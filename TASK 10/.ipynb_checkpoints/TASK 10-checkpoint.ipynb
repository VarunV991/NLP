{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization of Chinese Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinese language does not have any well defined word or sentence boundary as English has.\n",
    "#### This makes it harder for any system to convert the chinese texts into machine readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this task, I have used the Jieba library which is chinese segmentation module.\n",
    "# The algorithm behind this is that is based on Based on a prefix dictionary structure to achieve efficient word graph \n",
    "# scanning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a chinese news article on human rights\n",
    "article1 = '''国际人权服务社纽约办事处共同主任兼法律顾问马德琳·辛克莱尔指出，秘书长关于报复的年度报告中没有提到一些国家，\n",
    "那里的恐吓已经“发挥抑制作用，而民间社会很难或根本不可能与联合国接触。在其他国家，可能仍有报复案的报道，但这并不能说明全部情况，\n",
    "因为更多的人权捍卫者由于参与受到恐吓。这种现象本身就值得引起了关注，同时另一个令人担忧的问题是，在监督、\n",
    "记录和追究其责任方面存在着固有的困难。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Varun\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.124 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国际, 人权, 服务, 服务社, 纽约, 办事, 办事处, 共同, 主任, 兼, 法律, 顾问, 法律顾问, 马德琳, ·, 辛克, 克莱, 辛克莱, 尔, 指出, ，, 秘书, 秘书长, 关于, 报复, 的, 年度, 报告, 年度报告, 中, 没有, 提到, 一些, 国家, ，\n"
     ]
    }
   ],
   "source": [
    "# Taking first sentence\n",
    "article = article1.split('\\n')[0]\n",
    "\n",
    "# the input string is tokenized using .cut_for_search method\n",
    "tokens = jieba.cut_for_search(article)\n",
    "print(', '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['辛克莱', '马德琳', '服务社', '法律顾问', '年度报告']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keyword Extraction\n",
    "import jieba.analyse\n",
    "\n",
    "# Select top 5 frequently occuring words\n",
    "jieba.analyse.extract_tags(article, topK=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国际 n\n",
      "人权 n\n",
      "服务社 n\n",
      "纽约 ns\n",
      "办事处 n\n",
      "共同 d\n",
      "主任 b\n",
      "兼 v\n",
      "法律顾问 nr\n",
      "马德琳 nr\n",
      "· x\n",
      "辛克莱 nr\n",
      "尔 nr\n",
      "指出 v\n",
      "， x\n",
      "秘书长 n\n",
      "关于 p\n",
      "报复 v\n",
      "的 uj\n",
      "年度报告 n\n",
      "中 f\n",
      "没有 v\n",
      "提到 v\n",
      "一些 m\n",
      "国家 n\n",
      "， x\n"
     ]
    }
   ],
   "source": [
    "#POS Tagging\n",
    "\n",
    "# Segmenting the string according to the POS\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(article)\n",
    "for w in words:\n",
    "    print('%s %s' % (w.word, w.flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
